# DNN μ±„λ„ μ¶”μ • ν”„λ΅μ νΈ λ¶„μ„ - Claude λ©”λ¨λ¦¬

## π― ν”„λ΅μ νΈ κ°μ”
- **λ©μ **: 5G/6G ν†µμ‹ μ„ μ„ν• DNN κΈ°λ° DMRS μ±„λ„ μ¶”μ • μ‹μ¤ν…
- **ν•µμ‹¬ κΈ°μ **: LoRA(Low-Rank Adaptation) μ „μ΄ν•™μµμ„ ν†µν• ν¨μ¨μ  λ¨λΈ μ μ‘
- **νΉν™” ν™κ²½**: InF(Indoor Factory) μ±„λ„ ν™κ²½

## π—οΈ ν•µμ‹¬ μ•„ν‚¤ν…μ²

### λ¨λΈ κµ¬μ΅°
- **κΈ°λ° λ¨λΈ**: Transformer κΈ°λ° μ±„λ„ μ¶”μ •κΈ°
- **μ–΄ν…μ…**: Self-attention + Cross-attention
- **μ •κ·ν™”**: Pre-LayerNorm λ°©μ‹
- **μ…λ ¥ ν•νƒ**: (batch, 14_symbols, 3072_subcarriers, 2_channels)
- **μ¶λ ¥**: μ¶”μ • μ±„λ„ μ‘λ‹µ + λ³΄μƒλ μμ‹  μ‹ νΈ

### LoRA μ„¤μ •
```yaml
peft:
  peft_type: LORA
  r: 8                    # LoRA rank
  lora_alpha: 8          # scaling factor
  target_modules: ["mha_q_proj", "mha_k_proj", "mha_v_proj", "out_proj", "ffnn_linear1", "ffnn_linear2"]
  lora_dropout: 0.1
```

## π“ λ°μ΄ν„° κµ¬μ΅°

### μ±„λ„ νƒ€μ…
- **InF_Los_50000**: Indoor Factory Line-of-Sight (50,000 μƒν”)
- **InF_Nlos_50000**: Indoor Factory Non-Line-of-Sight (50,000 μƒν”)
- **RMa_Los**: Rural Macro (μ „μ΄ν•™μµ μ†μ¤)

### μ°Έμ΅° μ‹ νΈ μ„¤μ •
- **DMRS**: [0, 3072, 6] - μ²« λ²μ§Έ μ‹¬λ³Όμ 6κ°„κ²© μ„λΈμΊλ¦¬μ–΄
- **FFT ν¬κΈ°**: 4096
- **μ ν¨ μ„λΈμΊλ¦¬μ–΄**: 3072 (κ°€λ“ μ μ™Έ)
- **μ‹¬λ³Ό μ**: 14

## π”§ μ£Όμ” νμΌ κµ¬μ΅°

### ν•µμ‹¬ μ‹¤ν–‰ νμΌ
- **Transfer_v4.py**: LoRA μ „μ΄ν•™μµ λ©”μΈ μ—”μ§„
- **model/estimator_v4.py**: LoRA μ μ© κ°€λ¥ν• μ±„λ„ μ¶”μ • λ¨λΈ
- **model/transformer_v4.py**: λ¶„λ¦¬λ projection layer Transformer

### μ„¤μ • νμΌ
- **config/config_transfer_v4.yaml**: v4 λ¨λΈ μ „μ© μ„¤μ •
- **config/config_transfer_v4_InF.yaml**: InF ν™κ²½ νΉν™” μ„¤μ •
- **config/config_transfer_v4_RMa.yaml**: RMa ν™κ²½ νΉν™” μ„¤μ •

### λ°μ΄ν„° κ΄€λ ¨
- **dataset.py**: μ±„λ„ λ°μ΄ν„°μ…‹ λ΅λ” (PDP κΈ°λ°)
- **dataset/PDP_processed/**: μ „μ²λ¦¬λ μ±„λ„ PDP λ°μ΄ν„° (.mat)
- **sample_data_*/**: λ‹¤μ–‘ν• κ±°λ¦¬λ³„ μƒν” λ°μ΄ν„° (.npy, .npz)

## π€ ν›λ ¨ μ„¤μ •

### μµμ ν™” νλΌλ―Έν„°
```yaml
training:
  lr: 0.00001                    # λ‚®μ€ ν•™μµλ¥  (μ „μ΄ν•™μµ)
  num_iter: 200000              # μµλ€ μ΄ν„°λ μ΄μ…
  batch_size: 32
  use_scheduler: true           # Cosine Annealing
  max_norm: 1.0                # κ·Έλλ””μ–ΈνΈ ν΄λ¦¬ν•‘
```

### λ¨λΈ λ΅λ“ λ°©μ‹
- **pretrained**: μ‚¬μ „ ν›λ ¨λ `Large_estimator_PreLN.pt` λ΅λ“
- **finetune**: κΈ°μ΅΄ μ²΄ν¬ν¬μΈνΈμ—μ„ κ³„μ† ν•™μµ

## π“ μ„±λ¥ ν‰κ°€
- **μ†μ‹¤ ν•¨μ**: NMSE (Normalized Mean Square Error)
- **ν‰κ°€ μ§€ν‘**: μ±„λ„ NMSE, κ²€μ¦ NMSE
- **λ΅κΉ…**: Weights & Biases μ—°λ™
- **Early Stopping**: μ„ νƒμ  μ‚¬μ© κ°€λ¥

## π”„ μ „μ΄ν•™μµ μ „λµ

### λ‹¨κ³„λ³„ μ ‘κ·Ό
1. **κΈ°λ³Έ λ¨λΈ**: RMa ν™κ²½μ—μ„ μ‚¬μ „ ν›λ ¨
2. **LoRA μ μ©**: ν•µμ‹¬ attention/FFN λ μ΄μ–΄μ— μ μ©
3. **InF μ „μ΄**: InF λ°μ΄ν„°λ΅ LoRA νλΌλ―Έν„°λ§ ν•™μµ
4. **λ¨λΈ λ³‘ν•©**: μµμΆ… λ°°ν¬μ© ν†µν•© λ¨λΈ μƒμ„±

### μ €μ¥ λ°©μ‹
- **μ¤‘κ°„ μ €μ¥**: LoRA κ°€μ¤‘μΉλ§ λ³„λ„ μ €μ¥
- **μµμΆ… μ €μ¥**: λ³‘ν•©λ λ¨λΈμ„ engine.py νΈν™ ν•νƒλ΅ μ €μ¥

## π³ λ°°ν¬ ν™κ²½

### Docker μ„¤μ •
```bash
# μ‚¬μ „ κµ¬μ„±λ ν™κ²½
docker pull joowonoil/channel-estimation-env:latest
```

### ν•µμ‹¬ λΌμ΄λΈλ¬λ¦¬
- PyTorch 2.4.1 + CUDA 12.1
- transformers, peft (LoRA)
- tensorrt, onnx (μµμ ν™”)
- wandb (μ‹¤ν— κ΄€λ¦¬)

## β΅ μ¶”λ΅  μµμ ν™”
- **TensorRT λ³€ν™**: tensorrt_conversion_v4.py
- **ONNX μ§€μ›**: ν¬λ΅μ¤ ν”λ«νΌ λ°°ν¬
- **μ—”μ§„ νμΌ**: saved_model/μ—μ„ .engine νμΌ κ΄€λ¦¬

## π›οΈ μ‹¤ν–‰ λ…λ Ήμ–΄

### κΈ°λ³Έ ν›λ ¨
```bash
python Transfer_v4.py
```

### TensorRT λ³€ν™
```bash
python tensorrt_conversion_v4.py
```

### μ¶”λ΅  ν…μ¤νΈ
```bash
# DNN_channel_estimation_inference/ λ””λ ‰ν† λ¦¬μ—μ„
python compare.py
```

## π“ μ£Όμμ‚¬ν•­

### λ¨λΈ μ €μ¥ κ΄€λ ¨
- LoRA λ¨λΈ μ €μ¥ ν›„ requires_grad μƒνƒ λ³€ν™” μ£Όμ
- engine.py νΈν™μ„±μ„ μ„ν• state_dict λ³€ν™ ν•„μ”
- Early stopping μ‹ λ³‘ν•©λ λ¨λΈ μλ™ λ΅λ“

### λ°μ΄ν„° ν•νƒ
- λ³µμ†μ β†’ μ‹¤μλ¶€/ν—μλ¶€ λ¶„λ¦¬ μ²λ¦¬
- DMRS μΈλ±μ‹± λ°©μ‹ μΌκ΄€μ„± μ μ§€
- μ±„λ„ μ •κ·ν™”/μ—­μ •κ·ν™” μμ„ μ£Όμ

## π” λ””λ²„κΉ… ν¬μΈνΈ
- ν•™μµ κ°€λ¥ν• νλΌλ―Έν„° μ ν™•μΈ (LoRA μ μ© ν›„)
- κ·Έλλ””μ–ΈνΈ νλ¦„ μ΄μƒ κ°μ§€
- μ±„λ„ μ¶”μ • μ •ν™•λ„ μ‹κ°ν™” (wandb ν”λ΅―)
- λ©”λ¨λ¦¬ μ‚¬μ©λ‰ λ¨λ‹ν„°λ§

μ΄ ν”„λ΅μ νΈλ” **μ‹¤μ©μ μΈ 5G μ±„λ„ μ¶”μ •**μ„ μ„ν• **LoRA μ „μ΄ν•™μµ μ‹μ¤ν…**μΌλ΅, ν¨μ¨μ μΈ νλΌλ―Έν„° μ μ‘κ³Ό λ†’μ€ μ„±λ¥μ„ λ™μ‹μ— λ‹¬μ„±ν•λ” μ μ„¤κ³„λ μ•„ν‚¤ν…μ²μ…λ‹λ‹¤.